/** Spark application to calculate some statistics on event sessions.
  * Input should be generated by `SessionSqlApp` or `SessionAggregateApp`.
  *
  * Input format: `category,product,userId,eventTime,eventType,sessionStartTime,sessionEndTime,sessionId`
  *
  * Output format: `category,medianSessionDurationInMinutes,usersSpendingLessThan1Min,usersSpending1To5Min,usersSpendingMoreThan5Min`
  *
  */
object StatisticsSqlApp extends GenericApp {

  def appName = "statistics-sql-app"

  def execute(inputPath: String, outputPath: String) = withSpark { spark =>
    val sessions = spark.read
      .option("header", "true")
      .csv(inputPath)

    sessions.createTempView("sessions")

    val statistics = spark.sql(
      s"""select category,
        percentile(sessionDuration, 0.5) / 60 AS medianSessionDurationInMinutes,
        count(DISTINCT CASE WHEN sessionDuration < 60 THEN userId ELSE NULL END) AS usersSpendingLessThan1Min,
        count(DISTINCT CASE WHEN sessionDuration BETWEEN 60 and 300 THEN userId ELSE NULL END) AS usersSpending1To5Min,
        count(DISTINCT CASE WHEN sessionDuration > 300 THEN userId ELSE NULL END) AS usersSpendingMoreThan5Min
        FROM (SELECT userId, category,
                     unix_timestamp(cast(sessionEndTime AS TIMESTAMP)) -
                     unix_timestamp(cast(sessionStartTime AS TIMESTAMP)) AS sessionDuration
              FROM sessions
              GROUP BY userId, category, sessionId, sessionDuration)
        GROUP BY category""")
      .cache()

    statistics.write
      .option("header", "true")
      .csv(outputPath)

    statistics.show()
  }

  run()

}
